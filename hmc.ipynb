{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65327231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: OMEinsum loaded the CUDA module successfully\n",
      "└ @ OMEinsum C:\\Users\\andre\\.julia\\packages\\OMEinsum\\0C2IK\\src\\cueinsum.jl:117\n"
     ]
    }
   ],
   "source": [
    "using AdvancedHMC, Distributions, ForwardDiff, Zygote\n",
    "using LinearAlgebra\n",
    "using Flux\n",
    "using Distributions\n",
    "using Plots\n",
    "using OrderedCollections\n",
    "using Random\n",
    "using OMEinsum\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a31c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Finished 10 adapation steps\n",
      "│   adaptor = StanHMCAdaptor(\n",
      "    pc=WelfordVar,\n",
      "    ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.8, state.ϵ=0.001936231270987699),\n",
      "    init_buffer=75, term_buffer=50, window_size=25,\n",
      "    state=window(76, -40), window_splits()\n",
      ")\n",
      "│   κ.τ.integrator = Leapfrog(ϵ=0.00194)\n",
      "│   h.metric = DiagEuclideanMetric([1.0, 1.0, 1.0, 1.0, 1.0, 1 ...])\n",
      "└ @ AdvancedHMC C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\sampler.jl:189\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching EBFMI(::Vector{Any})\n\u001b[0mClosest candidates are:\n\u001b[0m  EBFMI(\u001b[91m::AbstractVector{<:Union{AbstractVector{var\"#s67\"}, var\"#s67\"} where var\"#s67\"<:AbstractFloat}\u001b[39m) at C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\diagnosis.jl:1",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching EBFMI(::Vector{Any})\n\u001b[0mClosest candidates are:\n\u001b[0m  EBFMI(\u001b[91m::AbstractVector{<:Union{AbstractVector{var\"#s67\"}, var\"#s67\"} where var\"#s67\"<:AbstractFloat}\u001b[39m) at C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\diagnosis.jl:1",
      "",
      "Stacktrace:",
      " [1] sample(rng::Random._GLOBAL_RNG, h::Hamiltonian{DiagEuclideanMetric{Float64, Vector{Float64}}, typeof(ℓπ), AdvancedHMC.var\"#∂ℓπ∂θ#66\"{typeof(ℓπ)}}, κ::HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}, θ::Vector{Float64}, n_samples::Int64, adaptor::StanHMCAdaptor{WelfordVar{Float64, Vector{Float64}}, NesterovDualAveraging{Float64}}, n_adapts::Int64; drop_warmup::Bool, verbose::Bool, progress::Bool, pm_next!::typeof(AdvancedHMC.pm_next!))",
      "   @ AdvancedHMC C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\sampler.jl:199",
      " [2] #sample#26",
      "   @ C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\sampler.jl:118 [inlined]",
      " [3] top-level scope",
      "   @ In[2]:230",
      " [4] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "begin\n",
    "    # Set the random seed for reproducible results. The random seed is used for random parameter initialisation and sampling of distributions.\n",
    "    Random.seed!(0)\n",
    "    \n",
    "    #= \n",
    "    This function separates the means and log standard deviations in a 1xD parameters vector.\n",
    "    The function simply splits the input vector into two, treats the first half as means, the second half as log standard deviations.\n",
    "    Input: 1xD vector of floats\n",
    "    Output: 1x(D/2) vector of means, 1x(D/2) vector of log standard deviations.\n",
    "    =#\n",
    "    function unpack_params(parameters)\n",
    "        mean, log_std = parameters[1:Int(length(parameters)/2)], parameters[Int(length(parameters)/2) + 1:end]\n",
    "        return mean, log_std\n",
    "    end\n",
    "\n",
    "    #= Function to specify, and produce the Bayesian Neural Network.\n",
    "    Input:\n",
    "        layer_sizes = vector of integers specifying number of nodes in each layer. Example: [1, 20, 20, 1]\n",
    "        L2_reg = a single float value specifying the L2 regularisation.\n",
    "        noise_variance = TODO\n",
    "        nonlinearity = function used as non-linearity between layers.\n",
    "        [TODO] add variable for final nonlinearity to add flexibility for final layer.\n",
    "    Output:\n",
    "        num_weights = total number of weights in the produced neural network.\n",
    "        predictions = forward pass function. \n",
    "        logprob = log probability function.\n",
    "    =#\n",
    "    function make_neural_network_functions(layer_sizes, L2_reg, noise_variance, nonlinearity)\n",
    "        shapes = collect(zip(layer_sizes[1:end-1], layer_sizes[2:end]))\n",
    "        num_weights = sum((m+1)*n for (m, n) in shapes)\n",
    "        function unpack_layers(weights)\n",
    "            #num_weights_sets = size(weights)[1]\n",
    "            W = OrderedDict()\n",
    "            b = OrderedDict()\n",
    "            i=1\n",
    "            for (m, n) in shapes\n",
    "                W[i] = reshape(weights[1:m*n], (m, n))\n",
    "                b[i] = reshape(weights[m*n:(m*n+n)-1], (1, n))\n",
    "                weights = weights[(m+1)*n:end]\n",
    "                i += 1\n",
    "            end\n",
    "            return (W, b)\n",
    "        end\n",
    "\n",
    "        # Outputs the predictions for each number of models sampled from posterior.\n",
    "        # inputs dimension: observations x features\n",
    "        # weights dimensions: \n",
    "        function predictions(weights, inputs)\n",
    "            #inputs = reshape(inputs, 1, size(inputs)...)\n",
    "            params = unpack_layers(weights)\n",
    "            Ws = params[1]\n",
    "            bs = params[2]\n",
    "            \n",
    "            # inputs_stacked = vcat()\n",
    "            # for i in range(1, size(weights)[1])\n",
    "            #     inputs_stacked = vcat(inputs_stacked, inputs)\n",
    "            # end\n",
    "            # inputs = inputs_stacked\n",
    "\n",
    "            #Go through all samples for each layer. (W,b) is the collection of all samples of weights and biases for a particular layer.\n",
    "            for j in range(1, length(Ws))\n",
    "                W = Ws[j]\n",
    "                b = bs[j]\n",
    "                outputs = ein\"ij,jk->ik\"(inputs, W) .+ b\n",
    "                inputs = nonlinearity(outputs)\n",
    "                if j == length(Ws)\n",
    "                    return outputs\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # This method returns a vector of length [mc_samples] containing the log probability value for each sample.\n",
    "        # weights = mc_samples x num_weights (non-variational)\n",
    "        # outputs = vector of likelihoods of size mc_samples\n",
    "        function logprob(weights, inputs, targets)\n",
    "            log_prior = -L2_reg * sum(weights.^2, dims=1)\n",
    "            preds = predictions(weights, inputs)\n",
    "            log_lik = -sum((preds .- targets).^2, dims=1)[:,1]./noise_variance\n",
    "            return log_prior[1] + log_lik[1]\n",
    "        end\n",
    "\n",
    "        return num_weights, predictions, logprob\n",
    "    end\n",
    "\n",
    "    # Synthetic regression dataset as provided in the Autograd blackbox svi in five lines of python paper.\n",
    "    function build_toy_dataset(n_data=40, noise_std=0.1)\n",
    "        D=1\n",
    "        inputs = vcat(LinRange(0,2,Int(n_data/2)), LinRange(6,8,Int(n_data/2)))\n",
    "        targets = cos.(inputs) .+ randn(n_data) .* noise_std\n",
    "        inputs = (inputs .- 4) ./4\n",
    "        inputs = reshape(inputs, (length(inputs),D))\n",
    "        targets = reshape(targets, (length(targets),D))\n",
    "        return inputs, targets\n",
    "    end\n",
    "\n",
    "\t# Same as dataset 1 but a more expanded version.\n",
    "    function build_toy_dataset_2(n_data=40, noise_std=0.1)\n",
    "        D=1\n",
    "        inputs = LinRange(0,32,Int(n_data))\n",
    "        targets = cos.(inputs) .+ randn(n_data) .* noise_std\n",
    "        inputs = (inputs .- 4) ./4\n",
    "        inputs = reshape(inputs, (length(inputs),D))\n",
    "        targets = reshape(targets, (length(targets),D))\n",
    "        return inputs, targets\n",
    "    end\n",
    "    \n",
    "    # Dataset from https://ekamperi.github.io/machine%20learning/2021/01/07/probabilistic-regression-with-tensorflow.html#tensorflow-example\n",
    "    function build_toy_dataset_3()\n",
    "        n_points = 100\n",
    "        x_train = collect(LinRange(-1, 1, n_points))\n",
    "        y_train = x_train.^5 + 0.4 .* x_train .* randn(n_points)\n",
    "        return reshape(x_train, (size(x_train)...,1)), reshape(y_train, (size(y_train)...,1))\n",
    "    end\n",
    "    \n",
    "    # Dataset from Lei's notebook.\n",
    "    function build_toy_dataset_4()\n",
    "        # generate the hidden signal \n",
    "        function signal_(x, k= -0.2)\n",
    "            if (x> -1) && (x < 1) \n",
    "                return 1\n",
    "            elseif (x>-8) && (x<-7)\n",
    "                return k * x + (1.6- k*(-8))\n",
    "            else\n",
    "                return 0\n",
    "            end\n",
    "        end\n",
    "        N = 50\n",
    "        σ² = 4*1e-2\n",
    "        # xsamples = rand(N) * 20 .- 10 \n",
    "        xsamples = collect(range(-10., 10., length=N))\n",
    "        # xsamples = shuffle(xsamples_)[1:N]\n",
    "        # generate targets ts or y\n",
    "        ts = signal_.(xsamples) + rand(Normal(0, sqrt(σ²)), N)\n",
    "        xsamples = reshape(xsamples, (length(xsamples),1))\n",
    "        ts = reshape(ts, (length(ts),1))\n",
    "        return xsamples, ts\n",
    "    end\n",
    "\n",
    "    # Create samples of weights based on variational parameters.\n",
    "    function sample_posteriors(variational_parameters)\n",
    "        means, log_stds = unpack_params(variational_parameters)\n",
    "        return randn(1, num_weights) .* exp.(log_stds)' .+ means'\n",
    "    end\n",
    "    \n",
    "    # Initialise variational parameters randomly.\n",
    "    function initialise_variational_parameters(num_weights)\n",
    "        init_mean = randn(num_weights)\n",
    "        init_log_std = -5 * ones(num_weights)\n",
    "        return vcat(init_mean, init_log_std)\n",
    "    end\n",
    "    \n",
    "    # Sample posteriors and plot predictions.\n",
    "    function sample_and_plot(init_var_params, number_of_models, title_name=\"\", ylims=(-3, 3), xlims=(-8, 8), xmin=-8, xmax=8, xrange=150)\n",
    "        plot_inputs = collect(LinRange(xmin, xmax, xrange))\n",
    "        plot_inputs = reshape(plot_inputs, (length(plot_inputs), 1))\n",
    "        scatter(inputs, targets, label=\"\")\n",
    "        for i in range(1, number_of_models)\n",
    "            sample_weights = sample_posteriors(init_var_params)\n",
    "            outs = predictions(sample_weights, plot_inputs)\n",
    "            plot!(plot_inputs, outs[1, :, 1], ylim=ylims, xlim=xlims, size=(700,400), label=\"\", title=title_name)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    function rbf(x)\n",
    "        exp.(-x.^2)\n",
    "    end\n",
    "    \n",
    "    function linear(x)\n",
    "        x\n",
    "    end\n",
    "    \n",
    "    function train_bayesian_neural_network(epochs, learning_rate, num_weights, objective)\n",
    "        init_var_params = initialise_variational_parameters(num_weights)\n",
    "        param_hist = Array{}[]\n",
    "        opt = ADAM(learning_rate)\n",
    "        elbos = zeros(epochs)\n",
    "        for i in range(1,epochs)\n",
    "            Flux.Optimise.update!(opt, init_var_params, gradient(objective, (init_var_params))[1])\n",
    "            push!(param_hist, copy(init_var_params))\n",
    "            elbos[i] = -objective(init_var_params)\n",
    "        end\n",
    "        return param_hist, elbos\n",
    "    end\n",
    "    \n",
    "    function animate_variational_params(param_hist, number_of_models, ylims=(-3, 3), xlims=(-8, 8), xmin=-8, xmax=8, xrange=150)\n",
    "        epochs = length(param_hist)\n",
    "        anim = @animate for i in range(1, epochs)\n",
    "            title=\"Iteration: \"*string(i)*\"/$epochs - Bayesian Neural Network\"\n",
    "            sample_and_plot(param_hist[i], number_of_models, title, ylims, xlims, xmin, xmax, xrange)\n",
    "        end\n",
    "        gif(anim, fps=20)\n",
    "    end\n",
    "\n",
    "\t# Specify a few constants\n",
    "\tRandom.seed!(0)\n",
    "\tmc_samples = 5\n",
    "\tnum_weights, predictions, logprob = make_neural_network_functions([1, 20, 20, 1], 0.1, 0.01, rbf)\n",
    "\tinputs, targets = build_toy_dataset()\n",
    "\tlog_posterior(weights) = logprob(weights, inputs, targets)\n",
    "\t\n",
    "\t# Choose parameter dimensionality and initial parameter value\n",
    "\tD = num_weights\n",
    "\tinitial_θ = rand(D)\n",
    "\t\n",
    "\t# Define the target distribution\n",
    "\tℓπ(θ) = log_posterior(θ)\n",
    "\t\n",
    "\t# Set the number of samples to draw and warmup iterations\n",
    "\t#20 10 showed results\n",
    "\tn_samples, n_adapts = 10, 10\n",
    "\t\n",
    "\t# Define a Hamiltonian system\n",
    "\tmetric = DiagEuclideanMetric(D)\n",
    "\thamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n",
    "\t\n",
    "\t# Define a leapfrog solver, with initial step size chosen heuristically\n",
    "\tinitial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\n",
    "\tintegrator = Leapfrog(initial_ϵ)\n",
    "\t\n",
    "\t# Define an HMC sampler, with the following components\n",
    "\t#   - multinomial sampling scheme,\n",
    "\t#   - generalised No-U-Turn criteria, and\n",
    "\t#   - windowed adaption for step-size and diagonal mass matrix\n",
    "\tproposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n",
    "\tadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n",
    "\t\n",
    "\t# Run the sampler to draw samples from the specified Gaussian, where\n",
    "\t#   - `samples` will store the samples\n",
    "\t#   - `stats` will store diagnostic statistics for each sample\n",
    "\tsamples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; progress=false, drop_warmup=true);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98f9b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: samples not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: samples not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ .\\In[3]:6",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "begin\n",
    "\t#Was capable of drawing straight from the posterior, but why are the first two draws really bad?\n",
    "\tplot_in = collect(LinRange(-5, 5, 100))\n",
    "\tplot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "\tepochs = length(samples)\n",
    "\tanim = @animate for i in range(1, epochs)\n",
    "\t\tscatter(inputs, targets, xlim=(-5, 5), ylim=(-5, 5), title=\"Hamiltonian Metropolis Hastings - Iteration $i/$epochs\")\n",
    "\t\tplot!(plot_in, predictions(samples[i], plot_in))\n",
    "\tend\n",
    "\tgif(anim, fps=3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb50ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: samples not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: samples not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[4]:4",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "plot_in = collect(LinRange(-5, 5, 100))\n",
    "plot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "epochs = length(samples)\n",
    "scatter(inputs, targets, xlim=(-5, 5), ylim=(-3, 3), title=\"Hamiltonian Metropolis Hastings - 5 Samples\", label=\"\")\n",
    "for i in range(1, epochs)\n",
    "    plot!(plot_in, predictions(samples[i], plot_in), label=\"\")\n",
    "end\n",
    "current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4599877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Finished 10 adapation steps\n",
      "│   adaptor = StanHMCAdaptor(\n",
      "    pc=WelfordVar,\n",
      "    ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.8, state.ϵ=0.0024953595499853315),\n",
      "    init_buffer=75, term_buffer=50, window_size=25,\n",
      "    state=window(76, -40), window_splits()\n",
      ")\n",
      "│   κ.τ.integrator = Leapfrog(ϵ=0.0025)\n",
      "│   h.metric = DiagEuclideanMetric([1.0, 1.0, 1.0, 1.0, 1.0, 1 ...])\n",
      "└ @ AdvancedHMC C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\sampler.jl:189\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching EBFMI(::Vector{Any})\n\u001b[0mClosest candidates are:\n\u001b[0m  EBFMI(\u001b[91m::AbstractVector{<:Union{AbstractVector{var\"#s67\"}, var\"#s67\"} where var\"#s67\"<:AbstractFloat}\u001b[39m) at C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\diagnosis.jl:1",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching EBFMI(::Vector{Any})\n\u001b[0mClosest candidates are:\n\u001b[0m  EBFMI(\u001b[91m::AbstractVector{<:Union{AbstractVector{var\"#s67\"}, var\"#s67\"} where var\"#s67\"<:AbstractFloat}\u001b[39m) at C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\diagnosis.jl:1",
      "",
      "Stacktrace:",
      " [1] sample(rng::Random._GLOBAL_RNG, h::Hamiltonian{DiagEuclideanMetric{Float64, Vector{Float64}}, typeof(ℓπ), AdvancedHMC.var\"#∂ℓπ∂θ#66\"{typeof(ℓπ)}}, κ::HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}, θ::Vector{Float64}, n_samples::Int64, adaptor::StanHMCAdaptor{WelfordVar{Float64, Vector{Float64}}, NesterovDualAveraging{Float64}}, n_adapts::Int64; drop_warmup::Bool, verbose::Bool, progress::Bool, pm_next!::typeof(AdvancedHMC.pm_next!))",
      "   @ AdvancedHMC C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\sampler.jl:199",
      " [2] #sample#26",
      "   @ C:\\Users\\andre\\.julia\\packages\\AdvancedHMC\\51xgc\\src\\sampler.jl:118 [inlined]",
      " [3] top-level scope",
      "   @ In[5]:37",
      " [4] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "\t# Specify a few constants\n",
    "\tRandom.seed!(0)\n",
    "\tmc_samples = 5\n",
    "\tnum_weights, predictions, logprob = make_neural_network_functions([1, 20, 20, 1], 0.1, 0.01, rbf)\n",
    "\tinputs, targets = build_toy_dataset_2()\n",
    "\tlog_posterior(weights) = logprob(weights, inputs, targets)\n",
    "\t\n",
    "\t# Choose parameter dimensionality and initial parameter value\n",
    "\tD = num_weights\n",
    "\tinitial_θ = rand(D)\n",
    "\t\n",
    "\t# Define the target distribution\n",
    "\tℓπ(θ) = log_posterior(θ)\n",
    "\t\n",
    "\t# Set the number of samples to draw and warmup iterations\n",
    "\t#20 10 showed results\n",
    "\tn_samples, n_adapts = 10, 10\n",
    "\t\n",
    "\t# Define a Hamiltonian system\n",
    "\tmetric = DiagEuclideanMetric(D)\n",
    "\thamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n",
    "\t\n",
    "\t# Define a leapfrog solver, with initial step size chosen heuristically\n",
    "\tinitial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\n",
    "\tintegrator = Leapfrog(initial_ϵ)\n",
    "\t\n",
    "\t# Define an HMC sampler, with the following components\n",
    "\t#   - multinomial sampling scheme,\n",
    "\t#   - generalised No-U-Turn criteria, and\n",
    "\t#   - windowed adaption for step-size and diagonal mass matrix\n",
    "\tproposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n",
    "\tadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n",
    "\t\n",
    "\t# Run the sampler to draw samples from the specified Gaussian, where\n",
    "\t#   - `samples` will store the samples\n",
    "\t#   - `stats` will store diagnostic statistics for each sample\n",
    "\tsamples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; progress=false, drop_warmup=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b613ccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: samples not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: samples not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ .\\In[6]:6",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "begin\n",
    "\t#Was capable of drawing straight from the posterior, but why are the first two draws really bad?\n",
    "\tplot_in = collect(LinRange(-5, 5, 100))\n",
    "\tplot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "\tepochs = length(samples)\n",
    "\tanim = @animate for i in range(1, epochs)\n",
    "\t\tscatter(inputs, targets, xlim=(-5, 5), ylim=(-5, 5), title=\"Hamiltonian Metropolis Hastings - Iteration $i/$epochs\")\n",
    "\t\tplot!(plot_in, predictions(samples[i], plot_in))\n",
    "\tend\n",
    "\tgif(anim, fps=3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee61560d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: samples not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: samples not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[7]:4",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "plot_in = collect(LinRange(-5, 5, 100))\n",
    "plot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "epochs = length(samples)\n",
    "scatter(inputs, targets, xlim=(-5, 5), ylim=(-3, 3), title=\"Hamiltonian Metropolis Hastings - 5 Samples\", label=\"\")\n",
    "for i in range(1, epochs)\n",
    "    plot!(plot_in, predictions(samples[i], plot_in), label=\"\")\n",
    "end\n",
    "current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f407e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Specify a few constants\n",
    "\tRandom.seed!(0)\n",
    "\tmc_samples = 5\n",
    "\tnum_weights, predictions, logprob = make_neural_network_functions([1, 20, 20, 1], 0.1, 0.01, rbf)\n",
    "\tinputs, targets = build_toy_dataset_3()\n",
    "\tlog_posterior(weights) = logprob(weights, inputs, targets)\n",
    "\t\n",
    "\t# Choose parameter dimensionality and initial parameter value\n",
    "\tD = num_weights\n",
    "\tinitial_θ = rand(D)\n",
    "\t\n",
    "\t# Define the target distribution\n",
    "\tℓπ(θ) = log_posterior(θ)\n",
    "\t\n",
    "\t# Set the number of samples to draw and warmup iterations\n",
    "\t#20 10 showed results\n",
    "\tn_samples, n_adapts = 10, 10\n",
    "\t\n",
    "\t# Define a Hamiltonian system\n",
    "\tmetric = DiagEuclideanMetric(D)\n",
    "\thamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n",
    "\t\n",
    "\t# Define a leapfrog solver, with initial step size chosen heuristically\n",
    "\tinitial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\n",
    "\tintegrator = Leapfrog(initial_ϵ)\n",
    "\t\n",
    "\t# Define an HMC sampler, with the following components\n",
    "\t#   - multinomial sampling scheme,\n",
    "\t#   - generalised No-U-Turn criteria, and\n",
    "\t#   - windowed adaption for step-size and diagonal mass matrix\n",
    "\tproposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n",
    "\tadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n",
    "\t\n",
    "\t# Run the sampler to draw samples from the specified Gaussian, where\n",
    "\t#   - `samples` will store the samples\n",
    "\t#   - `stats` will store diagnostic statistics for each sample\n",
    "\tsamples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; progress=false, drop_warmup=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\t#Was capable of drawing straight from the posterior, but why are the first two draws really bad?\n",
    "\tplot_in = collect(LinRange(-3, 3, 100))\n",
    "\tplot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "\tepochs = length(samples)\n",
    "\tanim = @animate for i in range(1, epochs)\n",
    "\t\tscatter(inputs, targets, xlim=(-3, 3), ylim=(-3, 3), title=\"Hamiltonian Metropolis Hastings - Iteration $i/$epochs\")\n",
    "\t\tplot!(plot_in, predictions(samples[i], plot_in))\n",
    "\tend\n",
    "\tgif(anim, fps=3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cdc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in = collect(LinRange(-3, 3, 100))\n",
    "plot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "epochs = length(samples)\n",
    "scatter(inputs, targets, xlim=(-3, 3), ylim=(-3, 3), title=\"Hamiltonian Metropolis Hastings - 5 Samples\", label=\"\")\n",
    "for i in range(1, epochs)\n",
    "    plot!(plot_in, predictions(samples[i], plot_in), label=\"\")\n",
    "end\n",
    "current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Specify a few constants\n",
    "\tRandom.seed!(0)\n",
    "\tmc_samples = 5\n",
    "\tnum_weights, predictions, logprob = make_neural_network_functions([1, 20, 20, 1], 0.1, 0.01, rbf)\n",
    "\tinputs, targets = build_toy_dataset_4()\n",
    "\tlog_posterior(weights) = logprob(weights, inputs, targets)\n",
    "\t\n",
    "\t# Choose parameter dimensionality and initial parameter value\n",
    "\tD = num_weights\n",
    "\tinitial_θ = rand(D)\n",
    "\t\n",
    "\t# Define the target distribution\n",
    "\tℓπ(θ) = log_posterior(θ)\n",
    "\t\n",
    "\t# Set the number of samples to draw and warmup iterations\n",
    "\t#20 10 showed results\n",
    "\tn_samples, n_adapts = 10, 10\n",
    "\t\n",
    "\t# Define a Hamiltonian system\n",
    "\tmetric = DiagEuclideanMetric(D)\n",
    "\thamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n",
    "\t\n",
    "\t# Define a leapfrog solver, with initial step size chosen heuristically\n",
    "\tinitial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\n",
    "\tintegrator = Leapfrog(initial_ϵ)\n",
    "\t\n",
    "\t# Define an HMC sampler, with the following components\n",
    "\t#   - multinomial sampling scheme,\n",
    "\t#   - generalised No-U-Turn criteria, and\n",
    "\t#   - windowed adaption for step-size and diagonal mass matrix\n",
    "\tproposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n",
    "\tadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n",
    "\t\n",
    "\t# Run the sampler to draw samples from the specified Gaussian, where\n",
    "\t#   - `samples` will store the samples\n",
    "\t#   - `stats` will store diagnostic statistics for each sample\n",
    "\tsamples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; progress=false, drop_warmup=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\t#Was capable of drawing straight from the posterior, but why are the first two draws really bad?\n",
    "\tplot_in = collect(LinRange(-15, 15, 100))\n",
    "\tplot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "\tepochs = length(samples)\n",
    "\tanim = @animate for i in range(1, epochs)\n",
    "\t\tscatter(inputs, targets, xlim=(-15, 15), ylim=(-3, 3), title=\"Hamiltonian Metropolis Hastings - Iteration $i/$epochs\")\n",
    "\t\tplot!(plot_in, predictions(samples[i], plot_in))\n",
    "\tend\n",
    "\tgif(anim, fps=3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in = collect(LinRange(-15, 15, 100))\n",
    "plot_in = reshape(plot_in, size(plot_in)..., 1)\n",
    "\n",
    "epochs = length(samples)\n",
    "scatter(inputs, targets, xlim=(-15, 15), ylim=(-3, 3), title=\"Hamiltonian Metropolis Hastings - 5 Samples\", label=\"\")\n",
    "for i in range(1, epochs)\n",
    "    plot!(plot_in, predictions(samples[i], plot_in), label=\"\")\n",
    "end\n",
    "current()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
